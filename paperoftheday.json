[
  {
    "title": "Auto-Lambda: Disentangling Dynamic Task Relationships",
    "link": "https://paperswithcode.com/paper/auto-lambda-disentangling-dynamic-task",
    "description": "Unlike previous methods where task relationships are assumed to be fixed, Auto-Lambda is a gradient-based meta learning framework which explores continuous, dynamic task relationships via task-specific weightings, and can optimise any choice of combination of tasks through the formulation of a meta-loss; where the validation loss automatically influences task weightings throughout training.",
    "date": "2/9/2022"
  },
  {
    "title": "PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX",
    "link": "https://paperswithcode.com/paper/pgmax-factor-graphs-for-discrete",
    "description": "PGMax is an open-source Python package for easy specification of discrete Probabilistic Graphical Models (PGMs) as factor graphs, and automatic derivation of efficient and scalable loopy belief propagation (LBP) implementation in JAX.",
    "date": "2/10/2022"
  },
  {
    "title": "Semi-Supervised Convolutive NMF for Automatic Music Transcription",
    "link": "https://paperswithcode.com/paper/semi-supervised-convolutive-nmf-for-automatic",
    "description": "Automatic Music Transcription, which consists in transforming an audio recording of a musical performance into symbolic format, remains a difficult Music Information Retrieval task.",
    "date": "2/11/2022"
  },
  {
    "title": "Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging",
    "link": "https://paperswithcode.com/paper/image-to-image-regression-with-distribution",
    "description": "Image-to-image regression is an important learning task, used frequently in biological imaging.",
    "date": "2/12/2022"
  },
  {
    "title": "The leap to ordinal: functional prognosis after traumatic brain injury using artificial intelligence",
    "link": "https://paperswithcode.com/paper/the-leap-to-ordinal-functional-prognosis",
    "description": "We analysed the effect of 2 design elements on ordinal model performance: (1) the baseline predictor set, ranging from a concise set of 10 validated predictors to a token-embedded representation of all possible predictors, and (2) the modelling strategy, from ordinal logistic regression to multinomial deep learning.",
    "date": "2/13/2022"
  },
  {
    "title": "SUPA: A Lightweight Diagnostic Simulator for Machine Learning in Particle Physics",
    "link": "https://paperswithcode.com/paper/supa-a-lightweight-diagnostic-simulator-for",
    "description": "Our contribution is SUPA, the SUrrogate PArticle propagation simulator, an algorithm and software package for generating data by simulating simplified particle propagation, scattering and shower development in matter.",
    "date": "2/14/2022"
  },
  {
    "title": "Source Code Summarization with Structural Relative Position Guided Transformer",
    "link": "https://paperswithcode.com/paper/source-code-summarization-with-structural",
    "description": "We further show that how the proposed SCRIPT captures the structural relative dependencies.",
    "date": "2/15/2022"
  },
  {
    "title": "A Pragmatic Machine Learning Approach to Quantify Tumor Infiltrating Lymphocytes in Whole Slide Images",
    "link": "https://paperswithcode.com/paper/a-pragmatic-machine-learning-approach-to",
    "description": "Our approach is to transfer an open source machine learning method for segmentation and classification of nuclei in H&E slides trained on public data to TIL quantification without manual labeling of our data.",
    "date": "2/16/2022"
  },
  {
    "title": "One Configuration to Rule Them All? Towards Hyperparameter Transfer in Topic Models using Multi-Objective Bayesian Optimization",
    "link": "https://paperswithcode.com/paper/one-configuration-to-rule-them-all-towards",
    "description": "Topic models are statistical methods that extract underlying topics from document collections.",
    "date": "2/17/2022"
  },
  {
    "title": "cosFormer: Rethinking Softmax in Attention",
    "link": "https://paperswithcode.com/paper/cosformer-rethinking-softmax-in-attention-1",
    "description": "As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length.",
    "date": "2/18/2022"
  }
]
